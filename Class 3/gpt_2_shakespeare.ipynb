{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "4dd6a5a8-4b76-4f9b-81ca-ef67f184bdf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=08ea57d3240a3535eb5841761e783365e58a18ca04359d214a601b28b6f131ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "74fe69b0-0322-4d44-8416-a013af7aa9d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 461Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 3.20Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 472Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:22, 21.7Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 555Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.28Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 5.05Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "45c432d7-6490-4e55-c7a1-5d5d24304b24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"is there heaven?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "c8cf5c4b-1e9d-4c52-a2c8-4b7526cdd9c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is there heaven?\n",
            "\n",
            "\"But what is the right of the Lord to command You to acknowledge this? And what is the right of the Lord to swear to the Lord that he will not deny you this, and that He will not deny you this?\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "a2fc4ee8-681f-4bdc-c5eb-53d15b5b2abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:49:13--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.16.94, 52.217.45.158, 52.217.204.168, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.16.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txt.1â€™\n",
            "\n",
            "\rnietzsche.txt.1       0%[                    ]       0  --.-KB/s               \rnietzsche.txt.1     100%[===================>] 586.82K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-22 12:49:13 (27.8 MB/s) - â€˜nietzsche.txt.1â€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "26a47f52-c6f4-452d-f690-32d37a084b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 194MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "de491930-4426-4c2b-957b-f3b95816f0f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:45:48--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-03-22 12:45:48 (181 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "0eb63ea1-ca28-4af6-fbe4-4529829fc8dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 7.09] loss=3.48 avg=3.48\n",
            "[2 | 9.15] loss=3.40 avg=3.44\n",
            "[3 | 11.23] loss=3.50 avg=3.46\n",
            "[4 | 13.32] loss=3.34 avg=3.43\n",
            "[5 | 15.40] loss=3.19 avg=3.38\n",
            "[6 | 17.49] loss=3.37 avg=3.38\n",
            "[7 | 19.58] loss=3.15 avg=3.35\n",
            "[8 | 21.68] loss=3.22 avg=3.33\n",
            "[9 | 23.78] loss=3.07 avg=3.30\n",
            "[10 | 25.90] loss=3.27 avg=3.30\n",
            "[11 | 28.00] loss=3.03 avg=3.27\n",
            "[12 | 30.13] loss=3.20 avg=3.26\n",
            "[13 | 32.25] loss=3.17 avg=3.26\n",
            "[14 | 34.37] loss=3.13 avg=3.25\n",
            "[15 | 36.49] loss=3.10 avg=3.24\n",
            "[16 | 38.64] loss=3.14 avg=3.23\n",
            "[17 | 40.77] loss=3.11 avg=3.22\n",
            "[18 | 42.92] loss=2.95 avg=3.21\n",
            "[19 | 45.07] loss=3.11 avg=3.20\n",
            "[20 | 47.21] loss=3.18 avg=3.20\n",
            "[21 | 49.36] loss=3.05 avg=3.19\n",
            "[22 | 51.51] loss=3.15 avg=3.19\n",
            "[23 | 53.66] loss=3.20 avg=3.19\n",
            "[24 | 55.80] loss=3.14 avg=3.19\n",
            "[25 | 57.94] loss=2.99 avg=3.18\n",
            "[26 | 60.09] loss=2.96 avg=3.17\n",
            "[27 | 62.24] loss=2.96 avg=3.16\n",
            "[28 | 64.38] loss=2.98 avg=3.15\n",
            "[29 | 66.52] loss=2.96 avg=3.15\n",
            "[30 | 68.67] loss=3.01 avg=3.14\n",
            "[31 | 70.81] loss=2.99 avg=3.13\n",
            "[32 | 72.96] loss=3.12 avg=3.13\n",
            "[33 | 75.11] loss=3.01 avg=3.13\n",
            "[34 | 77.25] loss=3.13 avg=3.13\n",
            "[35 | 79.40] loss=3.08 avg=3.13\n",
            "[36 | 81.54] loss=3.02 avg=3.12\n",
            "[37 | 83.69] loss=3.03 avg=3.12\n",
            "[38 | 85.84] loss=2.93 avg=3.12\n",
            "[39 | 87.98] loss=3.12 avg=3.12\n",
            "[40 | 90.13] loss=3.23 avg=3.12\n",
            "[41 | 92.28] loss=3.00 avg=3.12\n",
            "[42 | 94.43] loss=2.96 avg=3.11\n",
            "[43 | 96.58] loss=3.04 avg=3.11\n",
            "[44 | 98.74] loss=2.88 avg=3.10\n",
            "[45 | 100.90] loss=3.05 avg=3.10\n",
            "[46 | 103.06] loss=3.14 avg=3.10\n",
            "[47 | 105.22] loss=2.81 avg=3.09\n",
            "[48 | 107.38] loss=3.16 avg=3.10\n",
            "[49 | 109.56] loss=2.99 avg=3.09\n",
            "[50 | 111.73] loss=2.89 avg=3.09\n",
            "[51 | 113.90] loss=2.91 avg=3.08\n",
            "[52 | 116.08] loss=3.07 avg=3.08\n",
            "[53 | 118.26] loss=3.00 avg=3.08\n",
            "[54 | 120.44] loss=3.17 avg=3.08\n",
            "[55 | 122.62] loss=2.86 avg=3.08\n",
            "[56 | 124.81] loss=2.82 avg=3.07\n",
            "[57 | 127.00] loss=2.95 avg=3.07\n",
            "[58 | 129.20] loss=2.92 avg=3.07\n",
            "[59 | 131.40] loss=2.86 avg=3.06\n",
            "[60 | 133.60] loss=2.89 avg=3.06\n",
            "[61 | 135.82] loss=2.94 avg=3.06\n",
            "[62 | 138.03] loss=2.90 avg=3.05\n",
            "[63 | 140.25] loss=2.93 avg=3.05\n",
            "[64 | 142.47] loss=2.99 avg=3.05\n",
            "[65 | 144.70] loss=3.01 avg=3.05\n",
            "[66 | 146.94] loss=2.76 avg=3.04\n",
            "[67 | 149.17] loss=2.83 avg=3.04\n",
            "[68 | 151.42] loss=2.96 avg=3.04\n",
            "[69 | 153.67] loss=2.99 avg=3.03\n",
            "[70 | 155.93] loss=2.99 avg=3.03\n",
            "[71 | 158.19] loss=2.85 avg=3.03\n",
            "[72 | 160.47] loss=2.91 avg=3.03\n",
            "[73 | 162.74] loss=2.71 avg=3.02\n",
            "[74 | 165.02] loss=2.79 avg=3.02\n",
            "[75 | 167.30] loss=2.78 avg=3.01\n",
            "[76 | 169.58] loss=2.93 avg=3.01\n",
            "[77 | 171.86] loss=2.87 avg=3.01\n",
            "[78 | 174.11] loss=2.91 avg=3.01\n",
            "[79 | 176.37] loss=2.81 avg=3.00\n",
            "[80 | 178.62] loss=2.82 avg=3.00\n",
            "[81 | 180.87] loss=2.96 avg=3.00\n",
            "[82 | 183.12] loss=2.93 avg=3.00\n",
            "[83 | 185.36] loss=2.78 avg=2.99\n",
            "[84 | 187.59] loss=2.95 avg=2.99\n",
            "[85 | 189.83] loss=2.92 avg=2.99\n",
            "[86 | 192.05] loss=2.92 avg=2.99\n",
            "[87 | 194.28] loss=2.97 avg=2.99\n",
            "[88 | 196.51] loss=2.80 avg=2.99\n",
            "[89 | 198.73] loss=2.82 avg=2.98\n",
            "[90 | 200.96] loss=2.97 avg=2.98\n",
            "[91 | 203.18] loss=2.91 avg=2.98\n",
            "[92 | 205.39] loss=2.84 avg=2.98\n",
            "[93 | 207.61] loss=2.83 avg=2.98\n",
            "[94 | 209.83] loss=2.83 avg=2.98\n",
            "[95 | 212.05] loss=2.76 avg=2.97\n",
            "[96 | 214.27] loss=2.99 avg=2.97\n",
            "[97 | 216.49] loss=2.81 avg=2.97\n",
            "[98 | 218.71] loss=2.75 avg=2.97\n",
            "[99 | 220.94] loss=2.83 avg=2.96\n",
            "[100 | 223.17] loss=2.80 avg=2.96\n",
            "======== SAMPLE 1 ========\n",
            " had lost her. \"We had good news, my lady. I think you made a fine choice to send us back to the Free Cities.\" He bowed to her, his hand still in the girl's belly. \"Yes, of course, ma'am, I trust you and my friends will be as happy as never before, and, yes, we have some \n",
            "gods that may not yet be quite as eager to help us as I would have us. That said, you may be grateful for the good \n",
            "mail sent you by my brother, a man of your choosing, and you may have some things to say, if you find a way to say them. \n",
            "When we have gathered our supplies, you will speak with my Lord Commander and his council.\" He spoke at once, carefully \n",
            "while she gave him a single word. \"As you would wish. I hope you will be as patient as we were yesterday, \n",
            "or as you wished the week ended. Forgive us all if we do not give you the courage to reach all the way there will be. \n",
            "\"You, my lady, you will be among my first guests, in truth.\" \n",
            "My husband took the letter in hand. The rest of us sat beside him as the lord commander took his own seat. As they \n",
            "sent their letters, his command and the news of our success moved like lightning around the ship. I never knew \n",
            "this was so. I looked at my husband. \"What shall I say to these other men? Is it not to them to suffer shame or dishonor? \n",
            "What is to them to say to me that I have brought us a stranger, a fool, or worse? \n",
            "I am not a fool, my lord.\" He leaned forward, his face cold. \n",
            "Page 38\n",
            "\n",
            "\"You must be glad, Your Grace.\" I told her. \"Thank you, my lady. You can go back to your room, \n",
            "and if you wish, but first you will need to go back to your room to give us the word of your victory, and the \n",
            "death of your sister.\" \n",
            "\"Thank you, but don't think, my lord. We are all on the same side.\" His voice was warm and angry, \n",
            "unbreakable. \"Thank you, but don't imagine it, my lady: I am your bride, you are my bride, we \n",
            "have all been yours to bear, but I want to hear the truth from those who are not, and that is what \n",
            "will follow.\" He put his arms around her neck and squeezed her hand. \n",
            "She moaned in agony and went on, sobbing out, until we broke the silence. \n",
            "When we returned to our quarters, the Lord Commander looked at us through his closed window. \"How are you, \n",
            "my lady? You took my life. I am now your bride. You should have known to honor me at the altar.\" He lifted her \n",
            "hands, then kissed her and said, \"Thank you, my lord, but don't imagine it. You saw from your heart. \n",
            "You know what I will say to men as well as women. Remember, my \n",
            "Lady.\" He raised his left hand and began to stroke the back of her neck. \"I have promised you I will make \n",
            "one last visit to the Free Cities to be followed by a final farewell.\" His hand drew back over her breast. He looked \n",
            "at her tender-looking cheek, then back at his bride in the morning sun. No one else looked at him. \n",
            "\"My Lady, your love can never forget the love of my dear, Your Grace.\" He spoke up and went to bed. He \n",
            "sadly fell asleep while listening to the sound and seeing the stars. \n",
            "This has been his life's work. He is dead. He cannot live forever. He will be \n",
            "noted by his bride. \n",
            "JON OF THRONR O.C. DUNSON \n",
            "This morning my lord and lady began to sleep in her bed; I was not at home that day. \n",
            "I am afraid it's not my bed. \n",
            "I am afraid my wife would have no choice but to spend the rest of her life in bed, until she is a woman. It's hard \n",
            "to tell when she wants to turn into a man. Yet she has made my heart ache for her. The Lord Commander was in bed \n",
            "when she first came to me. \n",
            "\"There is something disturbing now,\" I told her. \"The moment I hear her saying she cannot have children, I feel \n",
            "that you must be careful of her.\" \n",
            "\"Your mother has given me no choice,\" was her answer to this. \"I am sure she is not the only one in your \n",
            "house, Your Grace. I had a few as you were leaving the Free\n",
            "\n",
            "[101 | 237.71] loss=2.89 avg=2.96\n",
            "[102 | 239.94] loss=2.77 avg=2.96\n",
            "[103 | 242.18] loss=2.80 avg=2.95\n",
            "[104 | 244.43] loss=2.92 avg=2.95\n",
            "[105 | 246.68] loss=2.78 avg=2.95\n",
            "[106 | 248.93] loss=3.00 avg=2.95\n",
            "[107 | 251.18] loss=2.68 avg=2.95\n",
            "[108 | 253.43] loss=2.64 avg=2.94\n",
            "[109 | 255.68] loss=2.85 avg=2.94\n",
            "[110 | 257.92] loss=2.88 avg=2.94\n",
            "[111 | 260.17] loss=2.98 avg=2.94\n",
            "[112 | 262.41] loss=2.54 avg=2.94\n",
            "[113 | 264.65] loss=2.82 avg=2.93\n",
            "[114 | 266.89] loss=2.77 avg=2.93\n",
            "[115 | 269.13] loss=2.84 avg=2.93\n",
            "[116 | 271.37] loss=2.77 avg=2.93\n",
            "[117 | 273.61] loss=2.72 avg=2.92\n",
            "[118 | 275.84] loss=2.84 avg=2.92\n",
            "[119 | 278.07] loss=2.78 avg=2.92\n",
            "[120 | 280.31] loss=2.62 avg=2.92\n",
            "[121 | 282.54] loss=2.78 avg=2.92\n",
            "[122 | 284.78] loss=2.71 avg=2.91\n",
            "[123 | 287.01] loss=2.76 avg=2.91\n",
            "[124 | 289.25] loss=2.87 avg=2.91\n",
            "[125 | 291.48] loss=2.77 avg=2.91\n",
            "[126 | 293.72] loss=2.73 avg=2.91\n",
            "[127 | 295.95] loss=2.58 avg=2.90\n",
            "[128 | 298.18] loss=2.88 avg=2.90\n",
            "[129 | 300.41] loss=2.62 avg=2.90\n",
            "[130 | 302.64] loss=2.65 avg=2.89\n",
            "[131 | 304.87] loss=2.65 avg=2.89\n",
            "[132 | 307.10] loss=2.71 avg=2.89\n",
            "[133 | 309.34] loss=2.60 avg=2.88\n",
            "[134 | 311.57] loss=2.72 avg=2.88\n",
            "[135 | 313.80] loss=2.83 avg=2.88\n",
            "[136 | 316.03] loss=2.80 avg=2.88\n",
            "[137 | 318.27] loss=2.74 avg=2.88\n",
            "[138 | 320.50] loss=2.61 avg=2.87\n",
            "[139 | 322.73] loss=2.80 avg=2.87\n",
            "[140 | 324.96] loss=2.84 avg=2.87\n",
            "[141 | 327.20] loss=2.67 avg=2.87\n",
            "[142 | 329.44] loss=2.66 avg=2.87\n",
            "[143 | 331.67] loss=2.58 avg=2.86\n",
            "[144 | 333.90] loss=2.65 avg=2.86\n",
            "[145 | 336.13] loss=2.88 avg=2.86\n",
            "[146 | 338.37] loss=2.74 avg=2.86\n",
            "[147 | 340.61] loss=2.66 avg=2.86\n",
            "[148 | 342.84] loss=2.56 avg=2.85\n",
            "[149 | 345.07] loss=2.66 avg=2.85\n",
            "[150 | 347.30] loss=2.71 avg=2.85\n",
            "[151 | 349.53] loss=2.75 avg=2.85\n",
            "[152 | 351.77] loss=2.61 avg=2.84\n",
            "[153 | 354.01] loss=2.74 avg=2.84\n",
            "[154 | 356.24] loss=2.59 avg=2.84\n",
            "[155 | 358.47] loss=2.75 avg=2.84\n",
            "[156 | 360.70] loss=2.64 avg=2.84\n",
            "[157 | 362.94] loss=2.71 avg=2.83\n",
            "[158 | 365.17] loss=2.70 avg=2.83\n",
            "[159 | 367.40] loss=2.60 avg=2.83\n",
            "[160 | 369.63] loss=2.63 avg=2.83\n",
            "[161 | 371.87] loss=2.74 avg=2.83\n",
            "[162 | 374.10] loss=2.72 avg=2.82\n",
            "[163 | 376.33] loss=2.66 avg=2.82\n",
            "[164 | 378.57] loss=2.51 avg=2.82\n",
            "[165 | 380.80] loss=2.69 avg=2.82\n",
            "[166 | 383.03] loss=2.59 avg=2.81\n",
            "[167 | 385.27] loss=2.53 avg=2.81\n",
            "[168 | 387.50] loss=2.74 avg=2.81\n",
            "[169 | 389.74] loss=2.71 avg=2.81\n",
            "[170 | 391.97] loss=2.64 avg=2.81\n",
            "[171 | 394.20] loss=2.63 avg=2.80\n",
            "[172 | 396.44] loss=2.41 avg=2.80\n",
            "[173 | 398.67] loss=2.75 avg=2.80\n",
            "[174 | 400.90] loss=2.53 avg=2.80\n",
            "[175 | 403.14] loss=2.61 avg=2.79\n",
            "[176 | 405.38] loss=2.63 avg=2.79\n",
            "[177 | 407.62] loss=2.59 avg=2.79\n",
            "[178 | 409.85] loss=2.63 avg=2.79\n",
            "[179 | 412.08] loss=2.56 avg=2.78\n",
            "[180 | 414.32] loss=2.52 avg=2.78\n",
            "[181 | 416.56] loss=2.60 avg=2.78\n",
            "[182 | 418.80] loss=2.81 avg=2.78\n",
            "[183 | 421.03] loss=2.55 avg=2.78\n",
            "[184 | 423.27] loss=2.58 avg=2.77\n",
            "[185 | 425.50] loss=2.59 avg=2.77\n",
            "[186 | 427.74] loss=2.55 avg=2.77\n",
            "[187 | 429.97] loss=2.61 avg=2.77\n",
            "[188 | 432.21] loss=2.65 avg=2.77\n",
            "[189 | 434.45] loss=2.55 avg=2.76\n",
            "[190 | 436.68] loss=2.58 avg=2.76\n",
            "[191 | 438.92] loss=2.60 avg=2.76\n",
            "[192 | 441.16] loss=2.59 avg=2.76\n",
            "[193 | 443.39] loss=2.70 avg=2.76\n",
            "[194 | 445.63] loss=2.48 avg=2.75\n",
            "[195 | 447.86] loss=2.62 avg=2.75\n",
            "[196 | 450.11] loss=2.49 avg=2.75\n",
            "[197 | 452.35] loss=2.57 avg=2.75\n",
            "[198 | 454.59] loss=2.42 avg=2.74\n",
            "[199 | 456.83] loss=2.55 avg=2.74\n",
            "[200 | 459.06] loss=2.43 avg=2.74\n",
            "======== SAMPLE 1 ========\n",
            " of an old blacksmith; the \n",
            "mummer would call the place a forge. \n",
            "He made the copper, tin, and lead from the bones of horses and a hundred others. He cut down the \n",
            "stocking \n",
            "Page 379\n",
            "\n",
            "stallions and a hundred blacksmiths and smiths and woodcutters, all in the same day's ride, to join the party. \n",
            "He made the walls, with the golds and silver and ironwork and armor and other things the crafters had left behind, so \n",
            "there was room for everybody. He painted the walls with green and red and gold and blue, he painted the stables with \n",
            "blue and red and green, he painted the houses, he painted the walls, the houses, the houses, the houses. \n",
            "He painted the windows with green and blue and gold and blue, and the walls with green and green and blue and \n",
            "red and gold and blue and red and red and red and red and red and red and red and red and red and red. \n",
            "He painted the houses with yellow, blue, red, white, red, yellow, black, or light grey . . . He painted the walls with \n",
            "somewhere between, green and golden. \n",
            "He painted the stables with green, blue, green, blue, red, and gold. And the walls with green and black. And the \n",
            "stables, and all the other wooden stables. And the stables are the only stables. Those are the only stables \n",
            "all the crafters can own. Only the crafters live in these stables. The crafters do not buy things. They live in these \n",
            "stables. He gave his son a piece of leather, a leather glove, two gold bracelets, a cloak, and so on. \n",
            "He took it away and gave it to his father. \"No more,\" he said when the leather was in hand. \"You are no more \n",
            "a crafter than a crafter takes a horse.\" \n",
            "Robert had not seen the leather yet, but he thought it was beautiful. \"I don't want to know,\" he said. \n",
            "\"So many words to the wise, to the wise,\" the boy told him. It was a good deal, and Robert was not stupid. He knew \n",
            "everything, he did not care for words much, but there was a certain kind of certainty to it, and the way \n",
            "him father strove to make him, he had a certain truth to it. \"My mother has had many a practice at deceit, \n",
            "her tongue still a gift to the court, and I do sometimes find the truth out, though I doubt it now, though I \n",
            "hopefully we won't be so foolish.\" The boy had a look across her cheek. At first she thought he smelled of \n",
            "gold, but she thought she had found that out herself. \"My father never gave a word of the times,\" she said in a tone that did not \n",
            "further concern her. \n",
            "Robert's father had gone to war in the Seven Kingdoms under Lord Randyll of Blackwater. Robert had served the \n",
            "King's Hand at King's Landing and the Vale of Arryn and beyond, and the war was fought at the Battle of Clydas, near \n",
            "Etos, in 494. It was there that Jon Arryn had his deathbed confession. Robert had murdered his uncle Barristan Selmy \n",
            "and his brother Barristan's queen, Barristan the Bold, in their bedchamber after the death of Bran on the Trident. In that \n",
            "time Robert had lived a long and bloody royal life. He had fought in the melee at Winterfell and the siege of \n",
            "the Wall, in the siege of King's Landing, in the siege of the Great Keep at the Trident, and \n",
            "in the march to the Trident. He had gone before them thousands of men in the Red Fork of the Trident, \n",
            "under the Black Tooth banner against the Free Men, in the line of knights, officers, and slaves. And he had \n",
            "never killed anybody. The boy was at his father's side, with his brother's ashes in their blood. Some day he might \n",
            "know who his killers were. \n",
            "He had to live with the truth of it. He was still an infant, Robert thought bitterly. The king had never \n",
            "broken his arm, he had fought more than once. There were nights and days that came and went and \n",
            "years that had to pass before he could be sure of his father's good graces, but he would never have broken \n",
            "a limb, never mind broken the other one, never mind been certain of the other one. \n",
            "He had to live and die and make his father's death some thing beyond his control. \n",
            "Robert would be proud, would be proud\n",
            "\n",
            "[201 | 472.26] loss=2.52 avg=2.74\n",
            "[202 | 474.50] loss=2.51 avg=2.73\n",
            "[203 | 476.74] loss=2.65 avg=2.73\n",
            "[204 | 478.97] loss=2.51 avg=2.73\n",
            "[205 | 481.21] loss=2.53 avg=2.73\n",
            "[206 | 483.44] loss=2.46 avg=2.72\n",
            "[207 | 485.68] loss=2.56 avg=2.72\n",
            "[208 | 487.92] loss=2.77 avg=2.72\n",
            "[209 | 490.16] loss=2.50 avg=2.72\n",
            "[210 | 492.39] loss=2.42 avg=2.72\n",
            "[211 | 494.62] loss=2.56 avg=2.72\n",
            "[212 | 496.86] loss=2.62 avg=2.71\n",
            "[213 | 499.10] loss=2.47 avg=2.71\n",
            "[214 | 501.34] loss=2.62 avg=2.71\n",
            "[215 | 503.57] loss=2.47 avg=2.71\n",
            "[216 | 505.81] loss=2.64 avg=2.71\n",
            "[217 | 508.05] loss=2.56 avg=2.71\n",
            "[218 | 510.29] loss=2.53 avg=2.70\n",
            "[219 | 512.53] loss=2.46 avg=2.70\n",
            "[220 | 514.76] loss=2.60 avg=2.70\n",
            "[221 | 517.00] loss=2.42 avg=2.70\n",
            "[222 | 519.23] loss=2.28 avg=2.69\n",
            "[223 | 521.47] loss=2.65 avg=2.69\n",
            "[224 | 523.71] loss=2.17 avg=2.69\n",
            "[225 | 525.95] loss=2.16 avg=2.68\n",
            "[226 | 528.19] loss=2.67 avg=2.68\n",
            "[227 | 530.42] loss=2.54 avg=2.68\n",
            "[228 | 532.66] loss=2.34 avg=2.67\n",
            "[229 | 534.91] loss=2.35 avg=2.67\n",
            "[230 | 537.15] loss=2.63 avg=2.67\n",
            "[231 | 539.38] loss=2.61 avg=2.67\n",
            "[232 | 541.62] loss=2.37 avg=2.67\n",
            "[233 | 543.85] loss=2.42 avg=2.66\n",
            "[234 | 546.09] loss=2.34 avg=2.66\n",
            "[235 | 548.33] loss=2.47 avg=2.66\n",
            "[236 | 550.58] loss=2.32 avg=2.65\n",
            "[237 | 552.81] loss=2.49 avg=2.65\n",
            "[238 | 555.05] loss=2.42 avg=2.65\n",
            "[239 | 557.29] loss=2.46 avg=2.65\n",
            "[240 | 559.53] loss=2.44 avg=2.64\n",
            "[241 | 561.77] loss=2.54 avg=2.64\n",
            "[242 | 564.01] loss=2.34 avg=2.64\n",
            "[243 | 566.24] loss=2.47 avg=2.64\n",
            "[244 | 568.48] loss=2.31 avg=2.63\n",
            "[245 | 570.72] loss=2.43 avg=2.63\n",
            "[246 | 572.96] loss=2.49 avg=2.63\n",
            "[247 | 575.19] loss=2.48 avg=2.63\n",
            "[248 | 577.43] loss=2.50 avg=2.63\n",
            "[249 | 579.67] loss=2.38 avg=2.63\n",
            "[250 | 581.90] loss=2.35 avg=2.62\n",
            "[251 | 584.15] loss=2.32 avg=2.62\n",
            "[252 | 586.38] loss=2.34 avg=2.62\n",
            "[253 | 588.62] loss=2.35 avg=2.61\n",
            "[254 | 590.85] loss=2.59 avg=2.61\n",
            "[255 | 593.09] loss=2.35 avg=2.61\n",
            "[256 | 595.33] loss=2.13 avg=2.60\n",
            "[257 | 597.56] loss=2.56 avg=2.60\n",
            "[258 | 599.80] loss=2.50 avg=2.60\n",
            "[259 | 602.03] loss=2.23 avg=2.60\n",
            "[260 | 604.26] loss=2.40 avg=2.60\n",
            "[261 | 606.50] loss=2.46 avg=2.60\n",
            "[262 | 608.74] loss=2.62 avg=2.60\n",
            "[263 | 610.98] loss=2.37 avg=2.59\n",
            "[264 | 613.22] loss=2.48 avg=2.59\n",
            "[265 | 615.45] loss=2.71 avg=2.59\n",
            "[266 | 617.69] loss=2.59 avg=2.59\n",
            "[267 | 619.93] loss=2.40 avg=2.59\n",
            "[268 | 622.16] loss=2.52 avg=2.59\n",
            "[269 | 624.39] loss=2.34 avg=2.59\n",
            "[270 | 626.63] loss=2.52 avg=2.59\n",
            "[271 | 628.86] loss=2.30 avg=2.58\n",
            "[272 | 631.10] loss=2.19 avg=2.58\n",
            "[273 | 633.35] loss=2.37 avg=2.58\n",
            "[274 | 635.58] loss=2.47 avg=2.58\n",
            "[275 | 637.81] loss=2.49 avg=2.58\n",
            "[276 | 640.05] loss=2.78 avg=2.58\n",
            "[277 | 642.28] loss=2.36 avg=2.58\n",
            "[278 | 644.52] loss=2.44 avg=2.57\n",
            "[279 | 646.76] loss=2.25 avg=2.57\n",
            "[280 | 648.99] loss=2.55 avg=2.57\n",
            "[281 | 651.22] loss=2.32 avg=2.57\n",
            "[282 | 653.46] loss=2.49 avg=2.57\n",
            "[283 | 655.70] loss=2.22 avg=2.56\n",
            "[284 | 657.94] loss=2.28 avg=2.56\n",
            "[285 | 660.18] loss=2.54 avg=2.56\n",
            "[286 | 662.42] loss=2.51 avg=2.56\n",
            "[287 | 664.65] loss=2.34 avg=2.56\n",
            "[288 | 666.89] loss=2.47 avg=2.56\n",
            "[289 | 669.12] loss=2.34 avg=2.55\n",
            "[290 | 671.36] loss=2.32 avg=2.55\n",
            "[291 | 673.59] loss=2.20 avg=2.55\n",
            "[292 | 675.83] loss=2.34 avg=2.55\n",
            "[293 | 678.06] loss=2.35 avg=2.54\n",
            "[294 | 680.31] loss=2.48 avg=2.54\n",
            "[295 | 682.54] loss=2.49 avg=2.54\n",
            "[296 | 684.78] loss=2.42 avg=2.54\n",
            "[297 | 687.01] loss=2.21 avg=2.54\n",
            "[298 | 689.25] loss=2.29 avg=2.53\n",
            "[299 | 691.49] loss=2.34 avg=2.53\n",
            "[300 | 693.73] loss=2.32 avg=2.53\n",
            "======== SAMPLE 1 ========\n",
            " blood she held, \n",
            "\"You ought to let me play the game now. That is my last choice. I can see how your daughter fares.\" She took \n",
            "the small, wooden board and flipped on the dice. \"I've sent you to foster a king. It pays too soon, you know. \n",
            "Will you take to it?\" \n",
            "\"No?\" \n",
            "\"No,\" she said. There was a twist in her tone. \"No, I am sorry. I know what the prince asks. I know where you \n",
            "spoke. I know him. I'm not afraid, I know what you're afraid of. I did not know who he is. I was \n",
            "thinking of Robert now, you know. He was just an old man the old man had been. It would not be fair. I know what he \n",
            "does. I knew what he did. Father. The king of Tully, the Hand of the King, Lord of the \n",
            "Aerys Targaryen's House. What did you do? Go out and hunt? Do you hear me? \n",
            "Well, I took up hunting. You stay here. I'll come back with something better. If you will, I'll \n",
            "make you take to it.\" \n",
            "\"I will,\" she said, \"if it is not you. If it is not Robert who tells me . . . if it is not you, then I \n",
            "will give you that. A crown, my sweet lady. A double crown, or even a high one, I will take with me whatever honors \n",
            "are owed me, but never as much, no more. And I want no more of hunting, that is all. That is all. That should be enough, \n",
            "no more hunting. Let me take you with me, then. All the more I say that I will take no more of hunting either, no more. Let me not \n",
            "start the game again. Let me not start it again, my sweet lord, and you will.\" She pushed the board away from her, pulling it \n",
            "Page 6\n",
            "\n",
            "together, the weight of it lifting out of beneath her, and taking the long way across the room. \n",
            "\"I will have a crown, a crown or no.\" \n",
            "\"You can take no more of hunting,\" she said, \"but you must not forget that you are my son, and I shall take no more of \n",
            "hunting either. I have many crowns I have, but I will not take them all away. I will keep the crown.\" \n",
            "\"Robert is not a champion,\" said Lysa Arryn. \"He is not even a knight. And you, little lady . . . you are only my son, my \n",
            "wife, and you have scarcely looked a boy but a boy now. Oh, child, how proud I feel. Your father has \n",
            "seen fit to send me back for you, so I shall hold you back once more. He will not see to it.\" She \n",
            "bewled and began to pull off her cloak. \"I'll have you dressed before my court, but first you must take your \n",
            "daughter to the court and send me to King's Landing. \n",
            "I'll have a letter from her back, I promise you will. It will say where you are to stay . . . tell it to me why . . .\" \n",
            "Lysa Arryn looked at her daughter suspiciously, but did not say a word. She sat on the high table across \n",
            "from the great headmaster. \"We may have a letter from your daughter, Lysa. Remember that you are as good as \n",
            "ever, Lysa, or she will never understand.\" \n",
            "\"It was your daughter whose letter I sent back,\" answered Lysa Arryn. \"I took her to you, and sent her \n",
            "to the gods, but I never sent her back.\" \n",
            "Lysa looked at her daughter. \"My daughter is only five. She can wait until I am old, and her future is no more than a \n",
            "half year away.\" \n",
            "\"A long time,\" snapped Lysa. \"Even a moment.\" \n",
            "The long table in front of her made her feel all warm and comfortable; not cold, but cozy. \n",
            "When she said \"a year,\" they both took in her eyes and smiled at each other, and the room was silent. \n",
            "\"Lysa,\" she said. \"So young. All this has happened now? Tell me, \" Lysa Arryn said, \n",
            "so softly, to make their acquaintance. I know you, I know you, and I know the others too, and I must . . .\" She \n",
            "huffed in her seat. \"I . . . it will not be the end of this. My realm and my world are only a dream, I can \n",
            "make a name\n",
            "\n",
            "[301 | 706.81] loss=2.41 avg=2.53\n",
            "[302 | 709.04] loss=2.26 avg=2.53\n",
            "[303 | 711.27] loss=2.28 avg=2.52\n",
            "[304 | 713.50] loss=2.21 avg=2.52\n",
            "[305 | 715.73] loss=2.27 avg=2.52\n",
            "[306 | 717.98] loss=2.43 avg=2.52\n",
            "[307 | 720.22] loss=2.26 avg=2.51\n",
            "[308 | 722.45] loss=2.42 avg=2.51\n",
            "[309 | 724.68] loss=2.31 avg=2.51\n",
            "[310 | 726.92] loss=2.33 avg=2.51\n",
            "[311 | 729.15] loss=2.24 avg=2.51\n",
            "[312 | 731.40] loss=2.24 avg=2.50\n",
            "[313 | 733.63] loss=2.29 avg=2.50\n",
            "[314 | 735.86] loss=2.45 avg=2.50\n",
            "[315 | 738.10] loss=2.31 avg=2.50\n",
            "[316 | 740.33] loss=2.34 avg=2.50\n",
            "[317 | 742.57] loss=2.23 avg=2.49\n",
            "[318 | 744.80] loss=2.13 avg=2.49\n",
            "[319 | 747.04] loss=2.24 avg=2.49\n",
            "[320 | 749.27] loss=2.10 avg=2.48\n",
            "[321 | 751.50] loss=2.02 avg=2.48\n",
            "[322 | 753.75] loss=2.34 avg=2.48\n",
            "[323 | 755.98] loss=2.31 avg=2.48\n",
            "[324 | 758.22] loss=2.49 avg=2.48\n",
            "[325 | 760.45] loss=2.28 avg=2.47\n",
            "[326 | 762.69] loss=2.20 avg=2.47\n",
            "[327 | 764.93] loss=2.31 avg=2.47\n",
            "[328 | 767.16] loss=2.37 avg=2.47\n",
            "[329 | 769.40] loss=2.05 avg=2.46\n",
            "[330 | 771.63] loss=2.15 avg=2.46\n",
            "[331 | 773.87] loss=2.44 avg=2.46\n",
            "[332 | 776.10] loss=2.31 avg=2.46\n",
            "[333 | 778.35] loss=2.19 avg=2.46\n",
            "[334 | 780.58] loss=2.15 avg=2.45\n",
            "[335 | 782.81] loss=2.07 avg=2.45\n",
            "[336 | 785.04] loss=2.28 avg=2.45\n",
            "[337 | 787.28] loss=2.42 avg=2.45\n",
            "[338 | 789.51] loss=2.22 avg=2.44\n",
            "[339 | 791.75] loss=2.20 avg=2.44\n",
            "[340 | 793.99] loss=2.08 avg=2.44\n",
            "[341 | 796.22] loss=2.24 avg=2.44\n",
            "[342 | 798.45] loss=2.18 avg=2.43\n",
            "[343 | 800.69] loss=2.20 avg=2.43\n",
            "[344 | 802.93] loss=2.38 avg=2.43\n",
            "[345 | 805.16] loss=2.42 avg=2.43\n",
            "[346 | 807.40] loss=2.03 avg=2.43\n",
            "[347 | 809.63] loss=2.00 avg=2.42\n",
            "[348 | 811.87] loss=2.17 avg=2.42\n",
            "[349 | 814.11] loss=2.36 avg=2.42\n",
            "[350 | 816.35] loss=2.00 avg=2.41\n",
            "[351 | 818.58] loss=2.23 avg=2.41\n",
            "[352 | 820.82] loss=2.23 avg=2.41\n",
            "[353 | 823.05] loss=2.29 avg=2.41\n",
            "[354 | 825.30] loss=2.27 avg=2.41\n",
            "[355 | 827.53] loss=1.83 avg=2.40\n",
            "[356 | 829.76] loss=2.05 avg=2.40\n",
            "[357 | 832.00] loss=2.16 avg=2.40\n",
            "[358 | 834.23] loss=2.31 avg=2.40\n",
            "[359 | 836.48] loss=2.07 avg=2.39\n",
            "[360 | 838.72] loss=2.04 avg=2.39\n",
            "[361 | 840.95] loss=2.36 avg=2.39\n",
            "[362 | 843.19] loss=1.82 avg=2.38\n",
            "[363 | 845.42] loss=2.27 avg=2.38\n",
            "[364 | 847.65] loss=2.27 avg=2.38\n",
            "[365 | 849.89] loss=2.20 avg=2.38\n",
            "[366 | 852.13] loss=2.13 avg=2.38\n",
            "[367 | 854.37] loss=2.03 avg=2.37\n",
            "[368 | 856.60] loss=2.18 avg=2.37\n",
            "[369 | 858.84] loss=2.32 avg=2.37\n",
            "[370 | 861.07] loss=2.00 avg=2.37\n",
            "[371 | 863.32] loss=1.99 avg=2.36\n",
            "[372 | 865.55] loss=2.07 avg=2.36\n",
            "[373 | 867.79] loss=2.35 avg=2.36\n",
            "[374 | 870.03] loss=2.12 avg=2.36\n",
            "[375 | 872.26] loss=2.08 avg=2.35\n",
            "[376 | 874.50] loss=1.99 avg=2.35\n",
            "[377 | 876.74] loss=2.18 avg=2.35\n",
            "[378 | 878.98] loss=2.01 avg=2.34\n",
            "[379 | 881.21] loss=2.38 avg=2.35\n",
            "[380 | 883.45] loss=2.12 avg=2.34\n",
            "[381 | 885.70] loss=2.13 avg=2.34\n",
            "[382 | 887.94] loss=2.27 avg=2.34\n",
            "[383 | 890.18] loss=1.88 avg=2.34\n",
            "[384 | 892.42] loss=2.17 avg=2.33\n",
            "[385 | 894.65] loss=2.19 avg=2.33\n",
            "[386 | 896.89] loss=1.82 avg=2.33\n",
            "[387 | 899.14] loss=2.09 avg=2.32\n",
            "[388 | 901.37] loss=2.31 avg=2.32\n",
            "[389 | 903.61] loss=2.13 avg=2.32\n",
            "[390 | 905.85] loss=2.05 avg=2.32\n",
            "[391 | 908.08] loss=1.97 avg=2.32\n",
            "[392 | 910.32] loss=2.07 avg=2.31\n",
            "[393 | 912.55] loss=2.06 avg=2.31\n",
            "[394 | 914.79] loss=2.01 avg=2.31\n",
            "[395 | 917.02] loss=2.02 avg=2.30\n",
            "[396 | 919.26] loss=2.04 avg=2.30\n",
            "[397 | 921.50] loss=1.76 avg=2.30\n",
            "[398 | 923.74] loss=1.85 avg=2.29\n",
            "[399 | 925.98] loss=2.40 avg=2.29\n",
            "[400 | 928.22] loss=1.81 avg=2.29\n",
            "======== SAMPLE 1 ========\n",
            "of?\" \n",
            "\"No,\" he said. \"I have done everything that I can.\" \n",
            "\"You want?\" Sam asked. \n",
            "Bran looked up at him; the black dog who was he, the black dog of his day, the one who was just \n",
            "enough. \"I do!\" he said, laughing. \"I can go anywhere! This little kennel is full of places to do \n",
            "animal things!\" He grinned, a good long yawn, and Bran thought he was going to be a big boy again. \n",
            "And then he remembered . . . And he remembered his father . . . \n",
            "Page 539\n",
            "\n",
            ". . . and how it was he was crowned. \n",
            "\"No thanks,\" Sam said. \"Not one bit.\" He lifted the black pup by the collar and carried him up a flight of steps to \n",
            "the solar. That was the end of the solar. \n",
            "The wind came whistling down, and Bran could feel a huge gout of fresh air rising behind him. The \n",
            "thunder ceased. \n",
            "\"I,\" he said. \"I'm scared.\" He lifted the kennel and let it tumble. Sam laughed. \n",
            "It took him half an hour before he was strong enough to reach the bedchamber. Sam took Bran's arms and carried \n",
            "them to the sill. It took him another hour, until Sam had the hang of it. He held it between his knees. \"A good \n",
            "knee,\" he told Sam. \n",
            "The kennel was filthy, but no more so than the one Bran had named the Cat Alley. Its doors were \n",
            "scattered all over the yard. From time to time Sam would come in and out of the windows in order to \n",
            "brush out the mud and light the candle. He always did that at night, in the dead of night, when he was \n",
            "cold, and Sam had to lean on the fire to close the blinds. Sam was older than Sam was, and Sam would \n",
            "pull the candle apart and light the candle alone, but in the afternoon the candle would not open, and when it \n",
            "became hard to read Sam would just close his eyes and go to sleep, still in Sam's hand. \n",
            "His bed was carpeted with dry leaves, and it did not feel warm against his arm. Sam never made any effort \n",
            "to dress in grey wool or velvet or whatever other clothing he could find. \n",
            "His father took an unruly goodly chunk of the yard. His lord father, his lord father, the Greatjon of \n",
            "the Shadow, Sam Hornwood, were the only names he had heard called in Old Nan's household. Father had called him \n",
            "\"Landon,\" but it was only Septberry who called him that. His lord father hated Septburrows too. They sat \n",
            "in their father's great oak mooring, their black eyes fixed on the yard. Sam went to camp every summer \n",
            "through the summer, and came back to camp more or less every summer. His father had asked him to one day \n",
            "join the Night's Watch, and even tried. It didn't seem to get any older. He never said the word to his brother \n",
            "who kept him waiting in the yard. \n",
            "His lord father kept his word, he thought; his sister was away at Winterfell by the time Sam was even \n",
            "grown. They were not the only ones in the yard, either. His father had called them Snow, Ghost, and Toad. They \n",
            "were bigger cats, Sam told himself, somehow they were somehow more fierce. Their mothers said so. He \n",
            "had to be one of the Shadowcats. \n",
            "\"When I was growing up, my father would say something like, 'Father, tell them my name is Sandor Clegane and \n",
            "they'll give me a black whip,' so I would make certain I'd gotten the right one,\" Sam told himself. The \n",
            "more he grew attached to Clegane, the more he wanted to be a swordsman. \n",
            "His sister did not like the sound of that . . . \n",
            "\"Even when you're not a Stark,\" Sam put in, \"you still know . . . sometimes . . sometimes, when things \n",
            "don't go your way, you just have to keep on fighting. This is no easy thing for a kid to come up with.\" \n",
            "His fingers fumbled at his belt, clumsy and clumsy and clumsy, yet somehow this came together in one powerful \n",
            "hundred thousandths of a heartbeat. A strong wind blew, and Sam pulled his weight from the bed. Over his shoulders \n",
            "flew down. The clang of steel rang at his fingers, and his lord father groaned. A deep sigh of relief, Sam \n",
            "shaking off the first blow and remembering that his brother Ben Stark was already safely in\n",
            "\n",
            "[401 | 941.41] loss=2.15 avg=2.29\n",
            "[402 | 943.64] loss=1.98 avg=2.28\n",
            "[403 | 945.88] loss=2.13 avg=2.28\n",
            "[404 | 948.12] loss=1.98 avg=2.28\n",
            "[405 | 950.35] loss=1.89 avg=2.28\n",
            "[406 | 952.59] loss=1.98 avg=2.27\n",
            "[407 | 954.81] loss=1.95 avg=2.27\n",
            "[408 | 957.05] loss=1.85 avg=2.26\n",
            "[409 | 959.29] loss=1.78 avg=2.26\n",
            "[410 | 961.52] loss=2.12 avg=2.26\n",
            "[411 | 963.76] loss=1.62 avg=2.25\n",
            "[412 | 965.99] loss=2.17 avg=2.25\n",
            "[413 | 968.23] loss=1.99 avg=2.25\n",
            "[414 | 970.47] loss=2.07 avg=2.25\n",
            "[415 | 972.71] loss=1.75 avg=2.24\n",
            "[416 | 974.94] loss=1.74 avg=2.24\n",
            "[417 | 977.17] loss=1.84 avg=2.23\n",
            "[418 | 979.40] loss=2.01 avg=2.23\n",
            "[419 | 981.63] loss=1.86 avg=2.23\n",
            "[420 | 983.88] loss=1.86 avg=2.22\n",
            "[421 | 986.11] loss=1.99 avg=2.22\n",
            "[422 | 988.35] loss=1.81 avg=2.22\n",
            "[423 | 990.58] loss=2.16 avg=2.22\n",
            "[424 | 992.82] loss=1.74 avg=2.21\n",
            "[425 | 995.06] loss=1.99 avg=2.21\n",
            "[426 | 997.30] loss=1.68 avg=2.20\n",
            "[427 | 999.54] loss=1.66 avg=2.20\n",
            "[428 | 1001.77] loss=1.98 avg=2.20\n",
            "[429 | 1004.01] loss=1.72 avg=2.19\n",
            "[430 | 1006.24] loss=1.66 avg=2.19\n",
            "[431 | 1008.48] loss=1.93 avg=2.18\n",
            "[432 | 1010.71] loss=2.25 avg=2.18\n",
            "[433 | 1012.95] loss=2.02 avg=2.18\n",
            "[434 | 1015.18] loss=1.82 avg=2.18\n",
            "[435 | 1017.41] loss=1.96 avg=2.18\n",
            "[436 | 1019.65] loss=1.74 avg=2.17\n",
            "[437 | 1021.89] loss=2.00 avg=2.17\n",
            "[438 | 1024.13] loss=2.01 avg=2.17\n",
            "[439 | 1026.36] loss=2.16 avg=2.17\n",
            "[440 | 1028.59] loss=1.83 avg=2.16\n",
            "[441 | 1030.83] loss=1.98 avg=2.16\n",
            "[442 | 1033.06] loss=1.81 avg=2.16\n",
            "[443 | 1035.30] loss=1.95 avg=2.16\n",
            "[444 | 1037.53] loss=2.04 avg=2.16\n",
            "[445 | 1039.77] loss=2.04 avg=2.15\n",
            "[446 | 1042.00] loss=1.82 avg=2.15\n",
            "[447 | 1044.25] loss=2.15 avg=2.15\n",
            "[448 | 1046.49] loss=2.15 avg=2.15\n",
            "[449 | 1048.72] loss=2.29 avg=2.15\n",
            "[450 | 1050.96] loss=2.02 avg=2.15\n",
            "[451 | 1053.19] loss=1.72 avg=2.15\n",
            "[452 | 1055.43] loss=1.84 avg=2.14\n",
            "[453 | 1057.67] loss=1.93 avg=2.14\n",
            "[454 | 1059.90] loss=1.94 avg=2.14\n",
            "[455 | 1062.13] loss=1.76 avg=2.14\n",
            "[456 | 1064.37] loss=1.77 avg=2.13\n",
            "[457 | 1066.61] loss=1.73 avg=2.13\n",
            "[458 | 1068.86] loss=1.60 avg=2.12\n",
            "[459 | 1071.09] loss=1.86 avg=2.12\n",
            "[460 | 1073.33] loss=1.69 avg=2.12\n",
            "[461 | 1075.56] loss=1.67 avg=2.11\n",
            "[462 | 1077.80] loss=1.94 avg=2.11\n",
            "[463 | 1080.04] loss=1.82 avg=2.11\n",
            "[464 | 1082.28] loss=1.69 avg=2.10\n",
            "[465 | 1084.51] loss=1.66 avg=2.10\n",
            "[466 | 1086.75] loss=1.64 avg=2.09\n",
            "[467 | 1088.98] loss=1.66 avg=2.09\n",
            "[468 | 1091.22] loss=1.86 avg=2.09\n",
            "[469 | 1093.46] loss=1.74 avg=2.08\n",
            "[470 | 1095.69] loss=1.65 avg=2.08\n",
            "[471 | 1097.92] loss=2.00 avg=2.08\n",
            "[472 | 1100.16] loss=1.84 avg=2.08\n",
            "[473 | 1102.39] loss=1.81 avg=2.07\n",
            "[474 | 1104.64] loss=2.05 avg=2.07\n",
            "[475 | 1106.88] loss=1.39 avg=2.07\n",
            "[476 | 1109.12] loss=1.84 avg=2.06\n",
            "[477 | 1111.35] loss=1.72 avg=2.06\n",
            "[478 | 1113.59] loss=1.89 avg=2.06\n",
            "[479 | 1115.82] loss=1.94 avg=2.06\n",
            "[480 | 1118.07] loss=1.91 avg=2.06\n",
            "[481 | 1120.30] loss=1.73 avg=2.05\n",
            "[482 | 1122.53] loss=1.88 avg=2.05\n",
            "[483 | 1124.77] loss=1.98 avg=2.05\n",
            "[484 | 1127.00] loss=1.71 avg=2.05\n",
            "[485 | 1129.24] loss=1.60 avg=2.04\n",
            "[486 | 1131.48] loss=1.63 avg=2.04\n",
            "[487 | 1133.71] loss=1.71 avg=2.03\n",
            "[488 | 1135.95] loss=1.77 avg=2.03\n",
            "[489 | 1138.19] loss=1.73 avg=2.03\n",
            "[490 | 1140.43] loss=1.72 avg=2.03\n",
            "[491 | 1142.67] loss=2.23 avg=2.03\n",
            "[492 | 1144.90] loss=1.68 avg=2.02\n",
            "[493 | 1147.14] loss=1.75 avg=2.02\n",
            "[494 | 1149.37] loss=1.68 avg=2.02\n",
            "[495 | 1151.61] loss=1.75 avg=2.02\n",
            "[496 | 1153.85] loss=1.70 avg=2.01\n",
            "[497 | 1156.08] loss=1.52 avg=2.01\n",
            "[498 | 1158.32] loss=1.45 avg=2.00\n",
            "[499 | 1160.56] loss=1.62 avg=2.00\n",
            "[500 | 1162.79] loss=1.91 avg=2.00\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"is there heaven?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d434b0b-4e4d-436c-a1c2-93b53e52c4bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is there heaven? \n",
            "The knight was taller than Arya would have liked, and the one on the far end of the bench looked \n",
            "more like Bran. Bran was as helpless as a baby, and the boy was jerking up like a \n",
            "puppet. Bran's legs were so weak he could only walk a few feet. Robb had Bran strapped to a \n",
            "bundle of old leather and old hair and scarves that had become his everyday wear. \n",
            "\"I won't go,\" Bran said. \"I won't go. I swear it, I have to go.\" \n",
            "\"Go,\" Robb said. \"You won't. You won't.\" \n",
            "Bran's legs were cramping so bad he could scarcely stand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "5154fe4d-c7b9-473b-9f48-c54e77bedfcf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "id": "g7K9X3K8TEwj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}